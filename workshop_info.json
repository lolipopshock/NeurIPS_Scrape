[{"workshop_id": "13154", "workshop_name": "AI for Humanitarian Assistance and Disaster Response", "workshop_link": "https://hadr.ai", "workshop_abstract": "Natural disasters are one of the oldest threats to not just individuals but to the societies they co-exist in. As a result, humanity has ceaselessly sought way to provide assistance to people in need after disasters have struck. Further, natural disasters are but a single, extreme example of the many possible humanitarian crises. Disease outbreak, famine, and oppression against disadvantaged groups can pose even greater dangers to people that have less obvious solutions.\nIn this proposed workshop, we seek to bring together the Artificial Intelligence (AI) and Humanitarian Assistance and Disaster Response (HADR) communities in order to bring AI to bear on real-world humanitarian crises.\nThrough this workshop, we intend to establish meaningful dialogue between the communities.\nBy the end of the workshop, the NeurIPS research community can come to understand the practical challenges of in aiding those in crisis, while the HADR can understand the landscape that is the state of art and practice in AI.\nThrough this, we seek to begin establishing a pipeline of transitioning the research created by the NeurIPS community to real-world humanitarian issues.\n\n", "speaker_info": [{"name": "Ritwik Gupta", "affilation": "Carnegie Mellon University Software Engineering Institute"}, {"name": "Robin Murphy", "affilation": "Texas A&M University"}, {"name": "Trevor Darrell", "affilation": "UC Berkeley"}, {"name": "Eric Heim", "affilation": "Carnegie Mellon University Software Engineering Institute"}, {"name": "Zhangyang Wang", "affilation": "TAMU"}, {"name": "Bryce Goodman", "affilation": "Defense Innovation Unit"}]}, {"workshop_id": "13155", "workshop_name": "Bayesian Deep Learning", "workshop_link": "http://bayesiandeeplearning.org", "workshop_abstract": "Extending on the workshop\u2019s success from the past 3 years, this workshop will study the developments in the field of Bayesian deep learning (BDL) over the past year. The workshop will be a platform to host the recent flourish of ideas using Bayesian approaches in deep learning, and using deep learning tools in Bayesian modelling. The program includes a mix of invited talks, contributed talks, and contributed posters. Future directions for the field will be debated in a panel discussion. \nThe BDL workshop was the second largest workshop at NIPS over the past couple of years, with last year\u2019s workshop seeing again a 2x increase in the number of submissions (136 accepted abstracts in total), attracting sponsorship from Google, Microsoft, Uber, and Qualcomm in the form of student travel awards.\nSpeakers (all confirmed):\n* Andrew Wilson\n* Deborah Marks\n* Jasper Snoek\n* Roger Grosse\n* Chelsea Finn\n* Yingzhen Li\n* Alexander Matthews\nWe would like to highlight the extraordinary gender balance this year with 3 of the 7 invited speakers women in ML.\nWorkshop summary:\nWhile deep learning has been revolutionary for machine learning, most modern deep learning models cannot represent their uncertainty nor take advantage of the well studied tools of probability theory. This has started to change following recent developments of tools and techniques combining Bayesian approaches with deep learning. The intersection of the two fields has received great interest from the community, with the introduction of new deep learning models that take advantage of Bayesian techniques, and Bayesian models that incorporate deep learning elements. Many ideas from the 1990s are now being revisited in light of recent advances in the fields of approximate inference and deep learning, yielding many exciting new results.\n\n", "speaker_info": [{"name": "Yarin Gal", "affilation": "University of Oxford"}, {"name": "Jose Miguel Hern\u00e1ndez-Lobato", "affilation": "University of Cambridge"}, {"name": "Christos Louizos", "affilation": "University of Amsterdam"}, {"name": "Eric Nalisnick", "affilation": "University of Cambridge"}, {"name": "Zoubin Ghahramani", "affilation": "Uber and University of Cambridge"}, {"name": "Kevin P Murphy", "affilation": "Google"}, {"name": "Max Welling", "affilation": "University of Amsterdam / Qualcomm AI Research"}]}, {"workshop_id": "13156", "workshop_name": "Beyond first order methods in machine learning systems", "workshop_link": "https://sites.google.com/site/optneurips19/", "workshop_abstract": "Optimization lies at the heart of many exciting developments in machine learning, statistics and signal processing. As models become more complex and datasets get larger, finding efficient, reliable and provable methods is one of the primary goals in these fields. \nIn the last few decades, much effort has been devoted to the development of first-order methods. These methods enjoy a low per-iteration cost and have optimal complexity, are easy to implement, and have proven to be effective for most machine learning applications. First-order methods, however, have significant limitations: (1) they require fine hyper-parameter tuning, (2) they do not incorporate curvature information, and thus are sensitive to ill-conditioning, and (3) they are often unable to fully exploit the power of distributed computing architectures. \nHigher-order methods, such as Newton, quasi-Newton and adaptive gradient descent methods, are extensively used in many scientific and engineering domains. At least in theory, these methods possess several nice features: they exploit local curvature information to mitigate the effects of ill-conditioning, they avoid or diminish the need for hyper-parameter tuning, and they have enough concurrency to take advantage of distributed computing environments. Researchers have even developed stochastic versions of  higher-order methods, that feature speed and scalability by incorporating curvature information in an economical and judicious manner. However, often higher-order methods are \u201cundervalued.\u201d\nThis workshop will attempt to shed light on this statement. Topics of interest include --but are not limited to-- second-order methods, adaptive gradient descent methods, regularization techniques, as well as techniques based on higher-order derivatives.\n\n", "speaker_info": [{"name": "Anastasios Kyrillidis", "affilation": "Rice University "}, {"name": "Albert Berahas", "affilation": "Lehigh University"}, {"name": "Farbod Roosta-Khorasani", "affilation": "University of Queensland"}, {"name": "Michael W Mahoney", "affilation": "UC Berkeley"}]}, {"workshop_id": "13157", "workshop_name": "Biological and Artificial Reinforcement Learning", "workshop_link": "https://sites.google.com/view/biologicalandartificialrl/", "workshop_abstract": "Reinforcement learning (RL) algorithms learn through rewards and a process of trial-and-error. This approach was strongly inspired by the study of animal behaviour and has led to outstanding achievements in machine learning (e.g. in games, robotics, science). However, artificial agents still struggle with a number of difficulties, such as sample efficiency, learning in dynamic environments and over multiple timescales, generalizing and transferring knowledge. On the other end, biological agents excel at these tasks. The brain has evolved to adapt and learn in dynamic environments, while integrating information and learning on different timescales and for different duration. Animals and humans are able to extract information from the environment in efficient ways by directing their attention and actively choosing what to focus on. They can achieve complicated tasks by solving sub-problems and combining knowledge as well as representing the environment in efficient ways and plan their decisions off-line. Neuroscience and cognitive science research has largely focused on elucidating the workings of these mechanisms. Learning more about the neural and cognitive underpinnings of these functions could be key to developing more intelligent and autonomous agents. Similarly, having a computational and theoretical framework, together with a normative perspective to refer to, could and does contribute to elucidate the mechanisms used by animals and humans to perform these tasks. Building on the connection between biological and artificial reinforcement learning, our workshop will bring together leading and emergent researchers from Neuroscience, Psychology and  Machine Learning to share: (i) how neural and cognitive mechanisms can provide insights to tackle challenges in RL research and (ii) how machine learning advances can help further our understanding of the brain and behaviour.\n\n", "speaker_info": [{"name": "Raymond Chua", "affilation": "McGill University"}, {"name": "Sara Zannone", "affilation": "ICL"}, {"name": "Feryal Behbahani", "affilation": "DeepMind"}, {"name": "Rui  Ponte Costa", "affilation": "University of Bristol"}, {"name": "Claudia Clopath", "affilation": "Imperial College London"}, {"name": "Blake Richards", "affilation": "University of Toronto"}, {"name": "Doina Precup", "affilation": "McGill University / DeepMind Montreal"}]}, {"workshop_id": "13158", "workshop_name": "Bridging Game Theory and Deep Learning", "workshop_link": "https://sgo-workshop.github.io/", "workshop_abstract": "Advances in generative modeling and adversarial learning gave rise to a recent surge of interest in differentiable two-players games, with much of the attention falling on generative adversarial networks (GANs). Solving these games introduces distinct challenges compared to the standard minimization tasks that the machine learning (ML) community is used to. A symptom of this issue is ML and deep learning (DL) practitioners using optimization tools on game-theoretic problems. Our NeurIPS 2018 workshop, \"Smooth games optimization in ML\", aimed to rectify this situation, addressing theoretical aspects of games in machine learning, their special dynamics, and typical challenges. For this year, we significantly expand our scope to tackle questions like the design of game formulations for other classes of ML problems, the integration of learning with game theory as well as their important applications. To that end, we have confirmed talks from \u00c9va Tardos, David Balduzzi and Fei Fang. We will also solicit contributed posters and talks in the area.\n\n", "speaker_info": [{"name": "Ioannis Mitliagkas", "affilation": "Mila & University of Montreal"}, {"name": "Gauthier Gidel", "affilation": "Mila"}, {"name": "Niao He", "affilation": "UIUC"}, {"name": "Reyhane Askari Hemmat", "affilation": "Mila & University of Montreal"}, {"name": "N H", "affilation": "CMU"}, {"name": "Simon Lacoste-Julien", "affilation": "Mila, Universit\u00e9 de Montr\u00e9al"}]}, {"workshop_id": "13159", "workshop_name": "Medical Imaging meets NeurIPS", "workshop_link": "https://sites.google.com/view/med-neurips-2019", "workshop_abstract": "Medical imaging and radiology are facing a major crisis with an ever-increasing complexity and volume of data along an immense economic pressure. The current advances and widespread use of imaging technologies now overload the human capacity of interpreting medical images, dangerously posing a risk of missing critical patterns of diseases. Machine learning has emerged as a key technology for developing novel tools in computer aided diagnosis, therapy and intervention. Still, progress is slow compared to other fields of visual recognition, which is mainly due to the domain complexity and constraints in clinical applications, i.e., robustness, high accuracy and reliability. \n\u201cMedical Imaging meets NeurIPS\u201d aims to bring researchers together from the medical imaging and machine learning communities to discuss the major challenges in the field and opportunities for research and novel applications. The proposed event will be the continuation of a successful workshop organized in NeurIPS 2017 and 2018 (https://sites.google.com/view/med-nips-2018). It will feature a series of invited speakers from academia, medical sciences and industry to give an overview of recent technological advances and remaining major challenges.\n\n", "speaker_info": [{"name": "Herv\u00e9 Lombaert", "affilation": "ETS Montreal / Inria"}, {"name": "Ben Glocker", "affilation": "Imperial College London"}, {"name": "Ender Konukoglu", "affilation": "ETH Zurich"}, {"name": "Marleen de Bruijne", "affilation": "Erasmus MC"}, {"name": "Aasa Feragen", "affilation": "University of Copenhagen, Denmark"}, {"name": "Ipek Oguz", "affilation": "Vanderbilt University"}, {"name": "Jonas Teuwen", "affilation": "Radboud University Medical Center / Netherlands Cancer Institute"}]}, {"workshop_id": "13160", "workshop_name": "Machine Learning with Guarantees", "workshop_link": "https://sites.google.com/view/mlwithguarantees", "workshop_abstract": "As adoption of machine learning grows in high-stakes application areas (e.g., industry, government and health care), so does the need for guarantees: how accurate a learned model will be; whether its predictions will be fair; whether it will divulge information about individuals; or whether it is vulnerable to adversarial attacks. Many of these questions involve unknown or intractable quantities (e.g., risk, regret or posterior likelihood) and complex constraints (e.g., differential privacy, fairness, and adversarial robustness). Thus, learning algorithms are often designed to yield (and optimize) bounds on the quantities of interest. Beyond providing guarantees, these bounds also shed light on black-box machine learning systems.\nClassical examples include structural risk minimization (Vapnik, 1991) and support vector machines (Cristianini & Shawe-Taylor, 2000), while more recent examples include non-vacuous risk bounds for neural networks (Dziugaite & Roy, 2017, 2018), algorithms that optimize both the weights and structure of a neural network (Cortes, 2017), counterfactual risk minimization for learning from logged bandit feedback (Swaminathan & Joachims, 2015; London & Sandler, 2019), robustness to adversarial attacks (Schmidt et al., 2018; Wong & Kolter, 2018), differentially private learning (Dwork et al., 2006, Chaudhuri et al., 2011), and algorithms that ensure fairness (Dwork et al., 2012).\nThis one-day workshop will bring together researchers in both theoretical and applied machine learning, across areas such as statistical learning theory, adversarial learning, fairness and privacy, to discuss the problem of obtaining performance guarantees and algorithms to optimize them. The program will include invited and contributed talks, poster sessions and a panel discussion. We particularly welcome contributions describing fundamentally new problems, novel learning principles, creative bound optimization techniques, and empirical studies of theoretical findings.\n\n", "speaker_info": [{"name": "Ben London", "affilation": "Amazon"}, {"name": "Gintare Karolina Dziugaite", "affilation": "Element AI & University of Cambridge"}, {"name": "Dan Roy", "affilation": "Univ of Toronto & Vector"}, {"name": "Thorsten Joachims", "affilation": "Cornell"}, {"name": "Aleksander Madry", "affilation": "MIT"}, {"name": "John Shawe-Taylor", "affilation": "UCL"}]}, {"workshop_id": "13161", "workshop_name": "Machine Learning for the Developing World (ML4D): Challenges and Risks", "workshop_link": "https://sites.google.com/view/ml4d/home", "workshop_abstract": "As the use of machine learning becomes ubiquitous, there is growing interest in understanding how machine learning can be used to tackle global development challenges. The possibilities are vast, and it is important that we explore the potential benefits of such technologies, which has driven the agenda of the ML4D workshop in the past. However, there is a risk that technology optimism and a categorization of ML4D research as inherently \u201csocial good\u201d may result in initiatives failing to account for unintended harms or deviating scarce funds towards initiatives that appear exciting but have no demonstrated effect. Machine learning technologies deployed in developing regions have often been created for different contexts and are trained with data that is not representative of the new deployment setting. Most concerning of all, companies sometimes make the deliberate choice to deploy new technologies in countries with little regulation in order to experiment.\nThis year\u2019s program will focus on the challenges and risks that arise when deploying machine learning in developing regions. This one-day workshop will bring together a diverse set of participants from across the globe to discuss essential elements for ensuring ML4D research moves forward in a responsible and ethical manner. Attendees will learn about potential unintended harms that may result from ML4D solutions, technical challenges that currently prevent the effective use of machine learning in vast regions of the world, and lessons that may be learned from other fields. \nThe workshop will include invited talks, a poster session of accepted papers and panel discussions. We welcome paper submissions featuring novel machine learning research that characterizes or tackle challenges of ML4D, empirical papers that reveal unintended harms of machine learning technology in developing regions, and discussion papers that examine the current state of the art of ML4D and propose paths forward.\n\n", "speaker_info": [{"name": "Maria De-Arteaga", "affilation": "Carnegie Mellon University"}, {"name": "Amanda Coston", "affilation": "Carnegie Mellon University"}, {"name": "Tejumade Afonja", "affilation": "InstaDeep"}]}, {"workshop_id": "13162", "workshop_name": " Machine Learning for Health (ML4H): What makes machine learning in medicine different?", "workshop_link": "https://ml4health.github.io/", "workshop_abstract": "The goal of the NeurIPS 2019 Machine Learning for Health Workshop (ML4H) is to foster collaborations that meaningfully impact medicine by bringing together clinicians, health data experts, and machine learning researchers. Attendees at this workshop can also expect to broaden their network of collaborators to include clinicians and machine learning researchers who are focused on solving some of the most import problems in medicine and healthcare. The organizers of this proposal have successfully run NeurIPS workshops in the past and are well-equipped to run this year\u2019s workshop should this proposal be accepted. \nThis year\u2019s theme of \u201cWhat makes machine learning in medicine different?\u201d aims to elucidate the obstacles that make the development of machine learning models for healthcare uniquely challenging. To speak to this theme, we have received commitments to speak from some of the leading researchers and physicians in this area. Below is a list of confirmed speakers who have agreed to participate.\nLuke Oakden-Raynor, MBBS (Adelaide)\nRuss Altman, MD/PhD (Stanford)\nLilly Peng, MD/PhD (Google) \nDaphne Koller, PhD (in sitro)\nJeff Dean, PhD (Google)\nAttendees at the workshop will gain an appreciation for problems that are unique to the application of machine learning for healthcare and a better understanding of how machine learning techniques may be leveraged to solve important clinical problems. This year\u2019s workshop builds on the last two NeurIPS ML4H workshops, which were both attended by more than 500 people each year, and helped form the foundations of an emerging research community. \nPlease see the attached document for the full program.\n\n", "speaker_info": [{"name": "Andrew Beam", "affilation": "Harvard Medical School"}, {"name": "Tristan Naumann", "affilation": "Microsoft Research"}, {"name": "Brett Beaulieu-Jones", "affilation": "Harvard Medical School"}, {"name": "Madalina Fiterau", "affilation": "CMU"}, {"name": "Irene Y Chen", "affilation": "MIT"}, {"name": "Sam G Finlayson", "affilation": "Harvard Medical School"}, {"name": "Emily Alsentzer", "affilation": "MIT"}]}, {"workshop_id": "13163", "workshop_name": "Machine Learning and the Physical Sciences", "workshop_link": "https://ml4physicalsciences.github.io/", "workshop_abstract": "Machine learning methods have had great success in learning complex representations that enable them to make predictions about unobserved data. Physical sciences span problems and challenges at all scales in the universe: from finding exoplanets in trillions of sky pixels, to finding machine learning inspired solutions to the quantum many-body problem, to detecting anomalies in event streams from the Large Hadron Collider. Tackling a number of associated data-intensive tasks including, but not limited to, segmentation, 3D computer vision, sequence modeling, causal reasoning, and efficient probabilistic inference are critical for furthering scientific discovery. In addition to using machine learning models for scientific discovery, the ability to interpret what a model has learned is receiving an increasing amount of attention.\nIn this targeted workshop, we would like to bring together computer scientists, mathematicians and physical scientists who are interested in applying machine learning to various outstanding physical problems, in particular in inverse problems and approximating physical processes; understanding what the learned model really represents; and connecting tools and insights from physical sciences to the study of machine learning models. In particular, the workshop invites researchers to contribute papers that demonstrate cutting-edge progress in the application of machine learning techniques to real-world problems in physical sciences, and using physical insights to understand what the learned model means.\nBy bringing together machine learning researchers and physical scientists who apply machine learning, we expect to strengthen the interdisciplinary dialogue, introduce exciting new open problems to the broader community, and stimulate production of new approaches to solving open problems in sciences. Invited talks from leading individuals in both communities will cover the state-of-the-art techniques and set the stage for this workshop.\n\n", "speaker_info": [{"name": "Anima Anandkumar", "affilation": "NVIDIA / Caltech"}, {"name": "Kyle Cranmer", "affilation": "New York University"}, {"name": "Roger G Melko", "affilation": "University of Waterloo / Perimeter Institute"}, {"name": "Mr. Prabhat", "affilation": "LBL/NERSC"}, {"name": "Frank Wood", "affilation": "University of British Columbia"}, {"name": "Atilim Gunes Baydin", "affilation": "University of Oxford"}, {"name": "Juan Carrasquilla", "affilation": ""}, {"name": "Shirley Ho", "affilation": "Flatiron Institute"}, {"name": "Karthik Kashinath", "affilation": "LBNL"}, {"name": "Michela Paganini", "affilation": "Facebook AI Research"}, {"name": "Savannah Thais", "affilation": "Yale University"}]}, {"workshop_id": "13173", "workshop_name": "Fair ML in Healthcare", "workshop_link": "http://www.fairmlforhealth.com", "workshop_abstract": "Clinical healthcare has been a natural application domain for ML with a few modest success stories of practical deployment. Inequity and healthcare disparity has long been a concern in clinical and public health for decades. However, the challenges of fair and equitable care using ML in health has largely remained unexplored. While a few works have attempted to highlight potential concerns and pitfalls in recent years, there are massive gaps in academic ML literature in this context. The goal of this workshop is to investigate issues around fairness that are specific to ML based healthcare. We hope to investigate a myriad of questions via the workshop.\n\n", "speaker_info": [{"name": "Shalmali Joshi", "affilation": "Vector Institute"}, {"name": "Irene Y Chen", "affilation": "MIT"}, {"name": "Ziad Obermeyer", "affilation": "UC Berkeley School of Public Health"}, {"name": "Sendhil Mullainathan", "affilation": "University of Chicago"}]}, {"workshop_id": "13165", "workshop_name": "Learning with Rich Experience: Integration of Learning Paradigms", "workshop_link": "https://sites.google.com/view/neurips2019lire", "workshop_abstract": "Machine learning is about computational methods that enable machines to learn concepts and improve performance from experience. Here, experience can take diverse forms, including data examples, abstract knowledge, interactions and feedback from the environment, other models, and so forth. Depending on different assumptions on the types and amount of experience available there are different learning paradigms, such as supervised learning, active learning, reinforcement learning, knowledge distillation, adversarial learning, and combinations thereof. On the other hand, a hallmark of human intelligence is the ability to learn from all sources of information. In this workshop, we aim to explore various aspects of learning paradigms, particularly theoretical properties and formal connections between them, and new algorithms combining multiple modes of supervisions, etc.\n\n", "speaker_info": [{"name": "Zhiting Hu", "affilation": "Carnegie Mellon University"}, {"name": "Andrew Wilson", "affilation": "Cornell University"}, {"name": "Chelsea Finn", "affilation": "Stanford"}, {"name": "Lisa Lee", "affilation": "Carnegie Mellon University"}, {"name": "Taylor Berg-Kirkpatrick", "affilation": "University of California San Diego"}, {"name": "Ruslan Salakhutdinov", "affilation": "Carnegie Mellon University"}, {"name": "Eric Xing", "affilation": "Petuum Inc. /  Carnegie Mellon University"}]}, {"workshop_id": "13166", "workshop_name": "Learning with Temporal Point Processes", "workshop_link": "https://sites.google.com/view/tpp-neurips-2019", "workshop_abstract": "In recent years, there has been an increasing number of machine learning models and algorithms based on the theory of temporal point processes, which is a mathematical framework to model asynchronous event data. These models and algorithm have found a wide range of human-centered applications, from social and information networks and recommender systems to crime prediction and health. Moreover, this emerging line of research has already established connections to deep learning, deep generative models, Bayesian nonparametrics, causal inference, stochastic optimal control and reinforcement learning. However, despite these recent advances, learning with temporal point processes is still a relatively niche topic within the machine learning community---there are only a few research groups across the world with the necessary expertise to make progress. In this workshop, we aim to popularize temporal point processes within the machine learning community at large. In our view, this is the right time to organize such a workshop because, as algorithmic decisions becomes more consequential to individuals and society, temporal point processes will play a major role on the development of human-centered machine learning models and algorithms accounting for the feedback loop between algorithmic and human decisions, which are inherently asynchronous events. Moreover, it will be a natural follow up of a very successful and well-attended ICML 2018 tutorial on learning with temporal point processes, which two of us recently taught.\n\n", "speaker_info": [{"name": "Manuel Rodriguez", "affilation": "MPI SWS"}, {"name": "Le Song", "affilation": "Ant Financial & Georgia Institute of Technology"}, {"name": "Isabel Valera", "affilation": "Max Planck Institute for Intelligent Systems"}, {"name": "Yan Liu", "affilation": "DiDi/University of Southern California"}, {"name": "Abir De", "affilation": "MPI-SWS"}, {"name": "Hongyuan Zha", "affilation": "Georgia Tech"}]}, {"workshop_id": "13167", "workshop_name": "Learning Transferable Skills", "workshop_link": "https://www.skillsworkshop.ai/", "workshop_abstract": "After spending several decades on the margin of AI, reinforcement learning has recently emerged as a powerful framework for developing intelligent systems that can solve complex tasks in real-world environments. This has had a tremendous impact on a wide range of tasks ranging from playing games such as Go and StarCraft to learning dexterity. However, one attribute of intelligence that still eludes modern learning systems is generalizability. Until very recently, the majority of reinforcement learning research involved training and testing algorithms on the same, sometimes deterministic, environment. This has resulted in algorithms that learn policies that typically perform poorly when deployed in environments that differ, even slightly, from those they were trained on. Even more importantly, the paradigm of task-specific training results in learning systems that scale poorly to a large number of (even interrelated) tasks.\nRecently there has been an enduring interest in developing learning systems that can learn transferable skills. This could mean robustness to changing environment dynamics, the ability to quickly adapt to environment and task variations or the ability to learn to perform multiple tasks at once (or any combination thereof). This interest has also resulted in a number of new data sets and challenges (e.g. Obstacle Tower Environment, Animal-AI, CoinRun) and an urgency to standardize the metrics and evaluation protocols to better assess the generalization abilities of novel algorithms. We expect this area to continue to increase in popularity and importance, but this can only happen if we manage to build consensus on which approaches are promising, and, equally important, how to test them.\nThe workshop will include a mix of invited speakers, accepted papers (oral and poster sessions) and a panel discussion. The workshop welcomes both theoretical and applied research, in addition to novel data sets and evaluation protocols. \n\n", "speaker_info": [{"name": "Marwan Mattar", "affilation": "Unity Technologies"}, {"name": "Arthur W Juliani", "affilation": "Unity Technologies"}, {"name": "Danny Lange", "affilation": "Unity Technologies"}, {"name": "Matthew Crosby", "affilation": "Imperial College London"}, {"name": "Benjamin Beyret", "affilation": "Imperial College London"}]}, {"workshop_id": "13168", "workshop_name": "Learning Meaningful Representations of Life", "workshop_link": "https://lmrl-bio.github.io", "workshop_abstract": "The last decade has seen both machine learning and biology transformed: the former by the ability to train complex predictors on massive labelled data sets; the latter by the ability to perturb and measure biological systems with staggering throughput, breadth, and resolution. In the next decade, machine learning and biomedicine are poised to alloy.\nHowever, fundamentally new ideas in machine learning are needed to translate biomedical data at scale into a mechanistic understanding of biology and disease at a level of abstraction beyond single genes. At the same time, the methodological advances necessary to realize this vision have the potential to drive the next decade of creativity in machine learning as the field grapples with how to move beyond prediction to a regime that broadly catalyzes and accelerates scientific discovery.\nTo seize this opportunity, we will bring together current and future leaders within each field to introduce the next generation of machine learnists to the next generation of biological problems. This full-day workshop will start a deeper dialogue between the two communities with the goal of Learning Meaningful Representations of Life (LMRL), emphasizing interpretable representation learning of structure and principles. We will address this problem at five layers of biological abstraction (genome, molecule, cell, system, and phenome), including through breakout sessions led by a diverse teams of biologists, machine learnists, and multilingual \u201cinterpreters\u201d familiar with both fields who can facilitate meaningful interaction.\nThe present moment holds incredible potential for deep reciprocity between fields. We would be thrilled to partner with NeurIPS to drive the impact of machine learning on biology, as well as biology on machine learning, toward positive and ethical ends.\n\n", "speaker_info": [{"name": "Jonathan Bloom", "affilation": "Broad Institute of MIT and Harvard"}]}, {"workshop_id": "13169", "workshop_name": "KR2ML - Knowledge Representation and Reasoning Meets Machine Learning", "workshop_link": "https://kr2ml.github.io/2019/", "workshop_abstract": "Machine learning (ML) has seen a tremendous amount of recent success and has been applied in a variety of applications. However, it comes with several drawbacks, such as the need for large amounts of training data and the lack of explainability and verifiability of the results. In many domains, there is structured knowledge (e.g., from electronic health records, laws, clinical guidelines, or common sense knowledge) which can be leveraged for reasoning in an informed way (i.e., including the information encoded in the knowledge representation itself) in order to obtain high quality answers. Symbolic approaches for knowledge representation and reasoning (KRR) are less prominent today - mainly due to their lack of scalability - but their strength lies in the verifiable and interpretable reasoning that can be accomplished. The KR2ML workshop aims at the intersection of these two subfields of AI. It will shine a light on the synergies that (could/should) exist between KRR and ML, and will initiate a discussion about the key challenges in the field.\n\n", "speaker_info": [{"name": "Veronika Thost", "affilation": "IBM Research"}, {"name": "Christian Muise", "affilation": "IBM Research AI"}, {"name": "Kartik Talamadupula", "affilation": "IBM Research"}, {"name": "Sameer  Singh", "affilation": "University of California, Irvine"}, {"name": "Chris R\u00e9", "affilation": "Stanford"}]}, {"workshop_id": "13170", "workshop_name": "Joint Workshop on AI for Social Good", "workshop_link": "https://aiforsocialgood.github.io/neurips2019/", "workshop_abstract": "The accelerating pace of intelligent systems research and real world deployment presents three clear challenges for producing \"good\" intelligent systems: (1) the research community lacks incentives and venues for results centered on social impact, (2) deployed systems often produce unintended negative consequences, and (3) there is little consensus for public policy that maximizes \"good\" social impacts, while minimizing the likelihood of harm. As a result, researchers often find themselves without a clear path to positive real world impact.\nThe Workshop on AI for Social Good addresses these challenges by bringing together machine learning researchers, social impact leaders, ethicists, and public policy leaders to present their ideas and applications for maximizing the social good. This workshop is a collaboration of three formerly separate lines of research (i.e., this is a \"joint\" workshop), including researchers in applications-driven AI research, applied ethics, and AI policy. Each of these research areas are unified into a 3-track framework promoting the exchange of ideas between the practitioners of each track.\nWe hope that this gathering of research talent will inspire the creation of new approaches and tools, provide for the development of intelligent systems benefiting all stakeholders, and converge on public policy mechanisms for encouraging these goals.\n\n", "speaker_info": [{"name": "Fei Fang", "affilation": "Carnegie Mellon University"}, {"name": "Joseph Bullock", "affilation": "UN Global Pulse and Durham University"}, {"name": "Marc-Antoine Dilhac", "affilation": "Universit\u00e9 de Montr\u00e9al"}, {"name": "Brian P Green", "affilation": "Santa Clara University"}, {"name": "natalie saltiel", "affilation": "MIT"}, {"name": "Dhaval Adjodah", "affilation": "MIT"}, {"name": "Jack Clark", "affilation": "OpenAI"}, {"name": "Sean McGregor", "affilation": "Syntiant and XPRIZE"}, {"name": "Margaux Luck", "affilation": "MILA"}, {"name": "Jonnie Penn", "affilation": "University of Cambridge"}, {"name": "Tristan Sylvain", "affilation": "MILA"}, {"name": "Genevi\u00e8ve Boucher", "affilation": "IRIC"}, {"name": "Sydney Swaine-Simon", "affilation": "District 3"}, {"name": "Girmaw Abebe Tadesse", "affilation": "University of Oxford"}, {"name": "Myriam C\u00f4t\u00e9", "affilation": "Mila"}, {"name": "Anna Bethke", "affilation": "Intel"}, {"name": "Yoshua Bengio", "affilation": "Mila"}]}, {"workshop_id": "13171", "workshop_name": "Information Theory and Machine Learning", "workshop_link": "https://sites.google.com/view/itml19", "workshop_abstract": "Information theory is deeply connected to two key tasks in machine learning: prediction and representation learning. Because of these connections, information theory has found wide applications in machine learning tasks, such as proving generalization bounds, certifying fairness and privacy, optimizing information content of unsupervised/supervised representations, and proving limitations to prediction performance. Conversely, progress in machine learning have been successfully applied to classical information theory tasks such as compression and transmission. \nThese recent progress have lead to new open questions and opportunities: to marry the simplicity and elegance of information theoretic analysis with the complexity of modern high dimensional machine learning setups. However, because of the diversity of information theoretic research, different communities often progress independently despite shared questions and tools. For example, variational bounds to mutual information are concurrently developed in information theory, generative model, and learning theory communities. \nThis workshop hopes to bring together researchers from different disciplines, identify common grounds, and spur discussion on how information theory can apply to and benefit from modern machine learning setups.\n\n", "speaker_info": [{"name": "Shengjia Zhao", "affilation": "Stanford University"}, {"name": "Jiaming Song", "affilation": "Stanford University"}, {"name": "Yanjun Han", "affilation": "Stanford University"}, {"name": "Kristy Choi", "affilation": "Stanford University"}, {"name": "Pratyusha Kalluri", "affilation": "Stanford University"}, {"name": "Ben Poole", "affilation": "Google Brain"}, {"name": "Alex Dimakis", "affilation": "University of Texas, Austin"}, {"name": "Jiantao Jiao", "affilation": "University of California, Berkeley"}, {"name": "Tsachy Weissman", "affilation": "Stanford University"}, {"name": "Stefano Ermon", "affilation": "Stanford"}]}, {"workshop_id": "13172", "workshop_name": "Graph Representation Learning", "workshop_link": "https://grlearning.github.io", "workshop_abstract": "Graph-structured data is ubiquitous throughout the natural and social sciences, from telecommunication networks to quantum chemistry. Building relational inductive biases into deep learning architectures is crucial if we want systems that can learn, reason, and generalize from this kind of data. Furthermore, graphs can be seen as a natural generalization of simpler kinds of structured data (such as images), and therefore, they represent a natural avenue for the next breakthroughs in machine learning.\nRecent years have seen a surge in research on graph representation learning, including techniques for deep graph embeddings, generalizations of convolutional neural networks to graph-structured data, and neural message-passing approaches inspired by belief propagation. These advances in graph neural networks and related techniques have led to new state-of-the-art results in numerous domains, including chemical synthesis, 3D-vision, recommender systems, question answering, and social network analysis. \nThe workshop will consist of contributed talks, contributed posters, and invited talks on a wide variety of methods and problems related to graph representation learning. We will welcome 4-page original research papers on work that has not previously been published in a machine learning conference or workshop.  In addition to traditional research paper submissions, we will also welcome 1-page submissions describing open problems and challenges in the domain of graph representation learning. These open problems will be presented as short talks (5-10 minutes) immediately preceding a coffee break to facilitate and spark discussions. \nThe primary goal for this workshop is to facilitate community building; with hundreds of new researchers beginning projects in this area, we hope to bring them together to consolidate this fast-growing area of graph representation learning into a healthy and vibrant subfield. \n\n", "speaker_info": [{"name": "Will Hamilton", "affilation": "McGill"}, {"name": "Rianne van den Berg", "affilation": "Google Brain"}, {"name": "Michael Bronstein", "affilation": "USI"}, {"name": "Stefanie Jegelka", "affilation": "MIT"}, {"name": "Thomas Kipf", "affilation": "University of Amsterdam"}, {"name": "Jure Leskovec", "affilation": "Stanford University and Pinterest"}, {"name": "Renjie Liao", "affilation": "University of Toronto"}, {"name": "Yizhou Sun", "affilation": "UCLA"}, {"name": "Petar Veli\u010dkovi\u0107", "affilation": "DeepMind"}]}, {"workshop_id": "13174", "workshop_name": "Emergent Communication: Towards Natural Language", "workshop_link": "https://sites.google.com/view/emecom2019", "workshop_abstract": "Communication is one of the most impressive human abilities but historically it has been studied in machine learning on confined datasets of natural language, and by various other fields in simple low-dimensional spaces. Recently, with the rise of deep RL methods, the questions around the emergence of communication can now be studied in new, complex multi-agent scenarios. Two previous successful workshops (2017, 2018) have gathered the community to discuss how, when, and to what end communication emerges, producing research that was later published at top ML venues such as ICLR, ICML, AAAI. Now, we wish to extend these ideas and explore a new direction: how emergent communication can become more like natural language, and what natural language understanding can learn from emergent communication.\nThe push towards emergent natural language is a necessary and important step in all facets of the field. For studying the evolution of human language, emerging a natural language can uncover the requirements that spurred crucial aspects of language (e.g. compositionality). When emerging communication for multi-agent scenarios, protocols may be sufficient for machine-machine interactions, but emerging a natural language is necessary for human-machine interactions. Finally, it may be possible to have truly general natural language understanding if agents learn the language through interaction as humans do. To make this progress, it is necessary to close the gap between artificial and natural language learning.\nTo tackle this problem, we want to take an interdisciplinary approach by inviting researchers from various fields (machine learning, game theory, evolutionary biology, linguistics, cognitive science, and programming languages) to participate and engaging them to unify the differing perspectives. We believe that the third iteration of this workshop with a novel, unexplored goal and strong commitment to diversity will allow this burgeoning field to flourish.\n\n", "speaker_info": [{"name": "Abhinav Gupta", "affilation": "Mila, Universit\u00e9 de Montr\u00e9al"}, {"name": "Michael Noukhovitch", "affilation": "Mila"}, {"name": "Cinjon Resnick", "affilation": "NYU"}, {"name": "Natasha Jaques", "affilation": "MIT"}, {"name": "Angelos Filos", "affilation": "University of Oxford"}, {"name": "Marie Ossenkopf", "affilation": "University of Kassel"}, {"name": "Angeliki Lazaridou", "affilation": "DeepMind"}, {"name": "Jakob Foerster", "affilation": "University of Oxford"}, {"name": "Ryan Lowe", "affilation": "McGill University"}, {"name": "Douwe Kiela", "affilation": "Facebook AI Research"}, {"name": "Kyunghyun Cho", "affilation": "NYU"}]}, {"workshop_id": "13175", "workshop_name": "EMC2: Energy Effcient Machine Learning and Cognitive Computing for Embedded Applications (5th edition)", "workshop_link": "https://www.emc2-workshop.com/neurips-19", "workshop_abstract": "Sample Call for Papers:\nA new wave of intelligent computing, driven by recent advances in machine learning and cognitive algorithms coupled\nwith process technology and new design methodologies, has the potential to usher unprecedented disruption in the way\nmodern computing systems are designed and deployed. These new and innovative approaches often provide an attractive\nand efficient alternative not only in terms of performance but also power, energy, and area. This disruption is easily visible\nacross the whole spectrum of computing systems -- ranging from low end mobile devices to large scale data centers and\nservers including intelligent infrastructures.\nA key class of these intelligent solutions is providing real-time, on-device cognition at the edge to enable many novel\napplications including computer vision and image processing, language understanding, speech and gesture recognition,\nmalware detection and autonomous driving. Naturally, these applications have diverse requirements for performance,\nenergy, reliability, accuracy, and security that demand a holistic approach to designing the hardware, software, and\nintelligence algorithms to achieve the best power, performance, and area (PPA).\nMore details in the attached pdf\n\n", "speaker_info": [{"name": "Raj Parihar", "affilation": "Cadence Design Systems"}, {"name": "Michael Goldfarb", "affilation": "Nvidia"}, {"name": "Satyam Srivastava", "affilation": "Intel"}]}, {"workshop_id": "13177", "workshop_name": "Deep Reinforcement Learning", "workshop_link": "https://sites.google.com/view/deep-rl-workshop-neurips-2019/home", "workshop_abstract": "In recent years, the use of deep neural networks as function approximators has enabled researchers to extend reinforcement learning techniques to solve increasingly complex control tasks. The emerging field of deep reinforcement learning has led to remarkable empirical results in rich and varied domains like robotics, strategy games, and multiagent interaction. This workshop will bring together researchers working at the intersection of deep learning and reinforcement learning, and it will help interested researchers outside of the field gain a high-level view about the current state of the art and potential directions for future contributions.\n\n", "speaker_info": [{"name": "Pieter Abbeel", "affilation": "UC Berkeley | Gradescope | Covariant"}, {"name": "Chelsea Finn", "affilation": "UC Berkeley"}, {"name": "Joelle Pineau", "affilation": ""}, {"name": "David Silver", "affilation": "DeepMind"}, {"name": "Satinder Singh", "affilation": "University of Michigan"}, {"name": "Joshua Achiam", "affilation": "UC Berkeley, OpenAI"}, {"name": "Carlos Florensa", "affilation": "UC Berkeley"}, {"name": "Christopher Grimm", "affilation": "University of Michigan"}, {"name": "Haoran Tang", "affilation": "UC Berkeley"}, {"name": "Vivek Veeriah", "affilation": "University of Michigan"}]}, {"workshop_id": "13178", "workshop_name": "Document Intelligence ", "workshop_link": "https://sites.google.com/view/di2019", "workshop_abstract": "Business documents are central to the operation of business. Such documents include sales agreements, vendor contracts, mortgage terms, loan applications, purchase orders, invoices, financial statements, employment agreements and a wide many more. The information in such business documents is presented in natural language, and can be organized in a variety of ways from straight text, multi-column formats, and a wide variety of tables. Understanding these documents is made challenging due to inconsistent formats, poor quality scans and OCR, internal cross references, and complex document structure. Furthermore, these documents often reflect complex legal agreements and reference, explicitly or implicitly, regulations, legislation, case law and standard business practices.\nThe ability to read, understand and interpret business documents, collectively referred to here as \u201cDocument Intelligence\u201d, is a critical and challenging application of artificial intelligence (AI) in business. While a variety of research has advanced the fundamentals of document understanding, the majority have focused on documents found on the web which fail to capture the complexity of analysis and types of understanding needed across business documents. Realizing the vision of document intelligence remains a research challenge that requires a multi-disciplinary perspective spanning not only natural language processing and understanding, but also computer vision, knowledge representation and reasoning, information retrieval, and more -- all of which have been profoundly impacted and advanced by neural network-based approaches and deep learning in the last few years.  \nWe propose to organize a workshop for AI researchers, academics and industry practitioners to discuss the opportunities and challenges for document intelligence. \n\n", "speaker_info": [{"name": "Nigel Duffy", "affilation": "EY"}, {"name": "Rama Akkiraju", "affilation": "IBM Research - Almaden"}, {"name": "Tania Bedrax Weiss", "affilation": "Google"}, {"name": "Paul Bennett", "affilation": "Microsoft Research"}, {"name": "Hamid Reza Motahari-Nezhad", "affilation": "EY AI Lab, USA"}]}, {"workshop_id": "13179", "workshop_name": "Context and Compositionality in Biological and Artificial Neural Systems", "workshop_link": "https://context-composition.github.io/", "workshop_abstract": "The ability to integrate semantic information across narratives is fundamental to language understanding in both biological and artificial cognitive systems. In recent years, enormous strides have been made in NLP and Machine Learning to develop architectures and techniques that effectively capture these effects. The field has moved away from traditional bag-of-words approaches that ignore temporal ordering, and instead embraced RNNs, Temporal CNNs and Transformers, which incorporate contextual information at varying timescales. While these architectures have lead to state-of-the-art performance on many difficult language understanding tasks, it is unclear what representations these networks learn and how exactly they incorporate context. Interpreting these networks, systematically analyzing the advantages and disadvantages of different elements, such as gating or attention, and reflecting on the capacity of the networks across various timescales are open and important questions. \nOn the biological side, recent work in neuroscience suggests that areas in the brain are organized into a temporal hierarchy in which different areas are not only sensitive to specific semantic information but also to the composition of information at different timescales. Computational neuroscience has moved in the direction of leveraging deep learning to gain insights about the brain. By answering questions on the underlying mechanisms and representational interpretability of these artificial networks, we can also expand our understanding of temporal hierarchies, memory, and capacity effects in the brain.  \nIn this workshop we aim to bring together researchers from machine learning, NLP, and neuroscience to explore and discuss how computational models should effectively capture the multi-timescale, context-dependent effects that seem essential for processes such as language understanding. \n\n", "speaker_info": [{"name": "Alan Yuille", "affilation": "Johns Hopkins University"}, {"name": "Leila Wehbe", "affilation": "Carnegie Mellon University"}, {"name": "Javier Turek", "affilation": "Brain-Inspired Computing Lab - Intel"}, {"name": "Emma Strubell", "affilation": "University of Massachusetts, Amherst"}, {"name": "Tal Linzen", "affilation": "Johns Hopkins University"}, {"name": "Shailee Jain", "affilation": "The University of Texas at Austin"}, {"name": "Alexander Huth", "affilation": "The University of Texas at Austin"}, {"name": "Christopher Honey", "affilation": "Johns Hopkins University"}, {"name": "Kyunghyun Cho", "affilation": "NYU"}]}, {"workshop_id": "13180", "workshop_name": "CiML 2019: Machine Learning Competitions for All", "workshop_link": "http://ciml.chalearn.org/", "workshop_abstract": "Challenges in machine learning and data science are open online competitions that address problems by providing datasets or simulated environments. They measure the performance of machine learning algorithms with respect to the given problem. The playful nature of challenges naturally attracts students, making challenges a great teaching resource. However, in addition to the use of challenges as educational tools, challenges have a role to play towards a better democratization of AI and machine learning. They function as cost effective problem-solving tools and a means of encouraging the development of re-usable problem templates and open-sourced solutions. However, at present, the geographic, sociological repartition of challenge participants and organizers is very biased. While recent successes in machine learning have raised much hopes, there is a growing concern that the societal and economical benefits might increasingly be in the power and under control of a few. \nCiML (Challenges in Machine Learning) is a forum that brings together workshop organizers, platform providers, and participants to discuss best practices in challenge organization and new methods and application opportunities to design high impact challenges. Following the success of previous years' workshops, we propose to reconvene and discuss new opportunities for broadening our community. \nFor this sixth edition of the CiML workshop at NeurIPS our objective is twofold: (1) We aim to enlarge the community, fostering diversity in the community of participants and organizers; (2) We aim to promote the organization of challenges for the benefit of more diverse communities.\nThe workshop will provide room for discussions on these topics, and aims to bring together potential partners to organize such challenges and stimulate \"machine learning for good\", i.e. the organization of challenges for the benefit of society. We have invited prominent speakers having experience in this domain.\n\n", "speaker_info": [{"name": "Adrienne Mendrik", "affilation": "Netherlands eScience Center"}, {"name": "Wei-Wei Tu", "affilation": "4Paradigm Inc."}, {"name": "Isabelle Guyon", "affilation": "UPSud, INRIA, University Paris-saclay and ChaLearn"}, {"name": "Evelyne Viegas", "affilation": "Microsoft Research"}]}, {"workshop_id": "13182", "workshop_name": "Minding the Gap: Between Fairness and Ethics", "workshop_link": "https://mindingthegap.github.io/", "workshop_abstract": "When researchers and practitioners, as well as policy makers and the public, discuss the impacts of deep learning systems, they draw upon multiple conceptual frames that do not sit easily beside each other. Questions of algorithmic fairness arise from a set of concerns that are similar, but not identical, to those that circulate around AI safety, which in turn overlap with, but are distinct from, the questions that motivate work on AI ethics, and so on. Robust bodies of research on privacy, security, transparency, accountability, interpretability, explainability, and opacity are also incorporated into each of these frames and conversations in variable ways. These frames reveal gaps that persist across both highly technical and socially embedded approaches, and yet collaboration across these gaps has proven challenging.\nFairness, Ethics, and Safety in AI each draw upon different disciplinary prerogatives, variously centering applied mathematics, analytic philosophy, behavioral sciences, legal studies, and the social sciences in ways that make conversation between these frames fraught with misunderstandings. These misunderstandings arise from a high degree of linguistic slippage between different frames, and reveal the epistemic fractures that undermine valuable synergy and productive collaboration. This workshop focuses on ways to translate between these ongoing efforts and bring them into necessary conversation in order to understand the profound impacts of algorithmic systems in society. \n\n", "speaker_info": [{"name": "Igor Rubinov", "affilation": "Dovetail Labs"}, {"name": "Risi Kondor", "affilation": "U. Chicago"}, {"name": "Jack Poulson", "affilation": "Tech Inquiry"}, {"name": "Manfred Warmuth", "affilation": "Univ. of Calif. at Santa Cruz"}, {"name": "Emanuel Moss", "affilation": "CUNY Graduate Center"}, {"name": "Alexa Hagerty", "affilation": "Dovetail Labs"}]}, {"workshop_id": "13183", "workshop_name": "MLSys: Workshop on Systems for ML", "workshop_link": "http://learningsys.org/neurips19/", "workshop_abstract": "A new area is emerging at the intersection of artificial intelligence, machine learning, and systems design. This has been accelerated by the explosive growth of diverse applications of ML in production, the continued growth in data volume, and the complexity of large-scale learning systems. The goal of this workshop is to bring together experts working at the crossroads of machine learning, system design and software engineering to explore the challenges faced when building large-scale ML systems. In particular, we aim to elicit new connections among these diverse fields, identifying theory, tools and design principles tailored to practical machine learning workflows.  We also want to think about best practices for research in this area and how to evaluate it. The workshop will cover state of the art ML and AI platforms and algorithm toolkits (e.g. TensorFlow, PyTorch1.0, MXNet etc.), as well as dive into machine learning-focused developments in distributed learning platforms, programming languages, data structures, hardware accelerators, benchmarking systems and other topics.\nThis workshop will follow the successful model we have previously run at ICML, NeurIPS and SOSP.\nOur plan is to run this workshop annually co-located with one ML venue and one Systems venue, to help build a strong community which we think will complement newer conferences like SysML targeting research at the intersection of systems and machine learning. We believe this dual approach will help to create a low barrier to participation for both communities.  \nThis workshop is part two of a two-part series with one day focusing on ML for Systems and the other on Systems for ML. Although the two workshops are being led by different organizers, we are coordinating our call for papers to ensure that the workshops complement each other and that submitted papers are routed to the appropriate venue. \n\n", "speaker_info": [{"name": "Aparna Lakshmiratan", "affilation": "Facebook"}, {"name": "Siddhartha Sen", "affilation": "Microsoft Research"}, {"name": "Joseph Gonzalez", "affilation": "UC Berkeley"}, {"name": "Dan Crankshaw", "affilation": "UC Berkeley"}, {"name": "Sarah Bird", "affilation": "Facebook"}]}, {"workshop_id": "13184", "workshop_name": "NeurIPS Workshop on Machine Learning for Creativity and Design 3.0", "workshop_link": "https://neurips2019creativity.github.io/", "workshop_abstract": "Generative machine learning and machine creativity have continued to grow and attract a wider audience to machine learning. Generative models enable new types of media creation across images, music, and text - including recent advances such as StyleGAN, MuseNet and GPT-2. This one-day workshop broadly explores issues in the applications of machine learning to creativity and design. We will look at algorithms for generation and creation of new media, engaging researchers building the next generation of generative models (GANs, RL, etc). We investigate the social and cultural impact of these new models, engaging researchers from HCI/UX communities and those using machine learning to develop new creative tools. In addition to covering the technical advances, we also address the ethical concerns ranging from the use of biased datasets to the use of synthetic media such as \u201cDeepFakes\u201d. Finally, we\u2019ll hear from some of the artists and musicians who are adopting machine learning including deep learning and reinforcement learning as part of their own artistic process.  We aim to balance the technical issues and challenges of applying the latest generative models to creativity and design with philosophical and cultural issues that surround this area of research.\n\n", "speaker_info": [{"name": "Luba Elliott", "affilation": "independent AI Curator"}, {"name": "Sander Dieleman", "affilation": "DeepMind"}, {"name": "Adam Roberts", "affilation": "Google Brain"}, {"name": "Jesse Engel", "affilation": "Google Brain"}, {"name": "Tom White", "affilation": "University of Wellington School of Design"}, {"name": "Rebecca Fiebrink", "affilation": "Goldsmiths University of London"}, {"name": "Parag K Mital", "affilation": "Kadenze, Inc."}, {"name": "Christine Payne", "affilation": "OpenAI"}, {"name": "Nao Tokui", "affilation": "Qosmo, Inc."}]}, {"workshop_id": "13185", "workshop_name": "Optimal Transport for Machine Learning", "workshop_link": "https://otml2019.github.io/", "workshop_abstract": "Optimal transport(OT) provides a powerful and flexible way to compare, interpolate and morph probability measures. Originally proposed in the eighteenth century, this theory later led to Nobel Prizes for Koopmans and Kantorovich as well as C. Villani and A. Figalli Fields\u2019 Medals in 2010 and 2018. OT is now used in challenging learning problems that involve high-dimensional data such as the inference of individual trajectories by looking at population snapshots in biology, the estimation of generative models for images, or more generally transport maps to transform samples in one space into another as in domain adaptation. With more than a hundred papers mentioning Wasserstein or transport in their title submitted at NeurIPS this year, and several dozens appearing every month acrossML/stats/imaging and data sciences, this workshop\u2019s aim will be to federate and advancecurrent knowledge in this rapidly growing field.\n\n", "speaker_info": [{"name": "Marco Cuturi", "affilation": "Google and CREST/ENSAE"}, {"name": "Gabriel Peyr\u00e9", "affilation": "CNRS and ENS"}, {"name": "R\u00e9mi Flamary", "affilation": "Universit\u00e9 C\u00f4te d'Azur"}, {"name": "Alexandra Suvorikova", "affilation": "U. Potsdam"}]}, {"workshop_id": "13198", "workshop_name": "Tackling Climate Change with ML", "workshop_link": "https://www.climatechange.ai/NeurIPS2019_workshop.html", "workshop_abstract": "Climate change is one of the greatest problems society has ever faced, with increasingly severe consequences for humanity as natural disasters multiply, sea levels rise, and ecosystems falter. Since climate change is a complex issue, action takes many forms, from designing smart electric grids to tracking greenhouse gas emissions through satellite imagery. While no silver bullet, machine learning can be an invaluable tool in fighting climate change via a wide array of applications and techniques. These applications require algorithmic innovations in machine learning and close collaboration with diverse fields and practitioners. This workshop is intended as a forum for those in the machine learning community who wish to help tackle climate change.\n\n", "speaker_info": [{"name": "David Rolnick", "affilation": "UPenn"}, {"name": "Alexandre Lacoste", "affilation": "Element AI"}, {"name": "Tegan Maharaj", "affilation": "MILA"}, {"name": "Priya Donti", "affilation": "Carnegie Mellon University"}, {"name": "Lynn Kaack", "affilation": "ETH Zurich"}, {"name": "John Platt", "affilation": "Google"}, {"name": "Jennifer Chayes", "affilation": "Microsoft Research"}, {"name": "Yoshua Bengio", "affilation": "Mila"}]}, {"workshop_id": "13186", "workshop_name": "Perception as generative reasoning: structure, causality, probability", "workshop_link": "https://pgr-workshop.wixsite.com/mysite", "workshop_abstract": "Many perception tasks can be cast as \u2018inverse problems\u2019 where the input signal is  the outcome of a causal process and perception is to invert that process. For example in visual object perception, the image is caused by an object and perception is to infer which object gave rise to that image. Following an analysis-by-synthesis approach, modelling the forward and causal direction of the data generation process is a natural way to capture the underlying scene structure, which typically leads to broader generalisation and better sample efficiency. Such a forward model can be applied to solve the inverse problem (inferring the scene structure from an input image) using Bayes rule, for example. This workflow stands in contrast to common approaches in deep learning, where typically one first defines a task, and then optimises a deep model end-to-end to solve it. In this workshop we propose to revisit ideas from the generative approach and advocate for learning-based analysis-by-synthesis methods for perception and inference. In addition, we pose the question of how ideas from these research areas can be combined with and complement modern deep learning practices.\n\n", "speaker_info": [{"name": "Dan Rosenbaum", "affilation": "DeepMind"}, {"name": "Marta Garnelo", "affilation": "DeepMind"}, {"name": "Peter Battaglia", "affilation": "DeepMind"}, {"name": "Kelsey Allen", "affilation": "MIT"}, {"name": "Ilker Yildirim", "affilation": "MIT"}]}, {"workshop_id": "13187", "workshop_name": "Privacy in Machine Learning (PiML)", "workshop_link": "https://priml-workshop.github.io/priml2019/", "workshop_abstract": "The goal of our workshop is to bring together privacy experts working in academia and industry to discuss the present and the future of privacy-aware technologies powered by machine learning. The workshop will focus on the technical aspects of privacy research and deployment with invited and contributed talks by distinguished researchers in the area. The programme of the workshop will emphasize the diversity of points of view on the problem of privacy. We will also ensure there is ample time for discussions that encourage networking between researches, which should result in mutually beneficial new long-term collaborations.\n\n", "speaker_info": [{"name": "Borja Balle", "affilation": "Amazon"}, {"name": "Kamalika Chaudhuri", "affilation": "UCSD"}, {"name": "Antti Honkela", "affilation": "University of Helsinki"}, {"name": "Antti Koskela", "affilation": "University of Helsinki"}, {"name": "Casey  Meehan", "affilation": "University of California, San Diego"}, {"name": "Mijung Park", "affilation": "MPI Tuebingen"}, {"name": "Mary Anne Smart", "affilation": "University of California, San Diego"}, {"name": "Adrian Weller", "affilation": "University of Cambridge"}]}, {"workshop_id": "13190", "workshop_name": "Retrospectives: A Venue for Self-Reflection in ML Research", "workshop_link": "https://ml-retrospectives.github.io/neurips2019/", "workshop_abstract": "The NeurIPS Workshop on Retrospectives in Machine Learning will kick-start the exploration of a new kind of scientific publication, called retrospectives. The purpose of a retrospective is to answer the question: \n\u201cWhat should readers of this paper know now, that is not in the original publication?\u201d \nRetrospectives provide a venue for authors to reflect on their previous publications, to talk about how their intuitions have changed, to identify shortcomings in their analysis or results, and to discuss resulting extensions that may not be sufficient for a full follow-up paper. A retrospective is written about a single paper, by that paper's author, and takes the form of an informal paper. The overarching goal of retrospectives is to improve the science, openness, and accessibility of the machine learning field, by widening what is publishable and helping to identifying opportunities for improvement. Retrospectives will also give researchers and practitioners who are unable to attend top conferences access to the author\u2019s updated understanding of their work, which would otherwise only be accessible to their immediate circle. \n\n", "speaker_info": [{"name": "Ryan Lowe", "affilation": "McGill University"}, {"name": "Yoshua Bengio", "affilation": "Mila"}, {"name": "Joelle Pineau", "affilation": "McGill University"}, {"name": "Michela Paganini", "affilation": "Facebook AI Research"}, {"name": "Jessica Forde", "affilation": "Project Jupyter"}, {"name": "Shagun Sodhani", "affilation": "MILA, University of Montreal"}, {"name": "Abhishek Gupta", "affilation": "Microsoft"}, {"name": "Joel Lehman", "affilation": "Uber AI Labs"}, {"name": "Peter Henderson", "affilation": "McGill University"}, {"name": "Kanika Madan", "affilation": "University of Toronto"}]}, {"workshop_id": "13189", "workshop_name": "Real Neurons & Hidden Units: future directions at the intersection of neuroscience and AI", "workshop_link": "https://sites.google.com/mila.quebec/neuroaiworkshop/", "workshop_abstract": "Recent years have witnessed an explosion of progress in AI. With it, a proliferation of experts and practitioners are pushing the boundaries of the field without regard to the brain. This is in stark contrast with the field's transdisciplinary origins, when interest in designing intelligent algorithms was shared by neuroscientists, psychologists and computer scientists alike. Similar progress has been made in neuroscience where novel experimental techniques now afford unprecedented access to brain activity and function. However, it is unclear how to maximize them to truly advance an end-to-end understanding of biological intelligence. The traditional neuroscience research program, however, lacks frameworks to truly advance an end-to-end understanding of biological intelligence. For the first time, mechanistic discoveries emerging from deep learning, reinforcement learning and other AI fields may be able to steer fundamental neuroscience research in ways beyond standard uses of machine learning for modelling and data analysis. For example, successful training algorithms in artificial networks, developed without biological constraints, can motivate research questions and hypotheses about the brain. Conversely, a deeper understanding of brain computations at the level of large neural populations may help shape future directions in AI. This workshop aims to address this novel situation by building on existing AI-Neuro relationships but, crucially, outline new directions for artificial systems and next-generation neuroscience experiments. We invite contributions concerned with the modern intersection between neuroscience and AI and in particular, addressing questions that can only now be tackled due to recent progress in AI on the role of recurrent dynamics, inductive biases to guide learning, global versus local learning rules, and interpretability of network activity. This workshop will promote discussion and showcase diverse perspectives on these open questions.\n\n", "speaker_info": [{"name": "Guillaume Lajoie", "affilation": "Universit\u00e9 de Montr\u00e9al / Mila"}, {"name": "Eli Shlizerman", "affilation": "Departments of Applied Mathematics and Electrical & Computer Engineering, University of Washington Seattle"}, {"name": "Maximilian Puelma Touzel", "affilation": "Mila"}, {"name": "Jessica Thompson", "affilation": "Universit\u00e9 de Montr\u00e9al"}, {"name": "Konrad Kording", "affilation": "Upenn"}]}, {"workshop_id": "13192", "workshop_name": "Robust AI in Financial Services: Data, Fairness, Explainability, Trustworthiness, and Privacy", "workshop_link": "", "workshop_abstract": "The financial services industry has unique needs for robustness when adopting artificial intelligence and machine learning (AI/ML). Many challenges can be described as intricate relationships between algorithmic fairness, explainability, privacy, data management, and trustworthiness. For example, there are ethical and regulatory needs to prove that models used for activities such as credit decisioning and lending are fair and unbiased, or that machine reliance does not cause humans to miss critical pieces of data. The use and protection of customer data necessitates secure and privacy-aware computation, as well as explainability around the use of sensitive data. Some challenges like entity resolution are exacerbated because of scale,  highly nuanced data points and missing information.\nOn top of these fundamental requirements, the financial industry is ripe with adversaries who purport fraud, resulting in large-scale data breaches and loss of confidential information in the financial industry. The need to counteract malicious actors therefore calls for robust methods that can tolerate noise and adversarial corruption of data. However, recent advances in adversarial attacks of AI/ML systems demonstrate how often generic solutions for robustness and security fail, thus highlighting the need for further advances. The challenge of robust AI/ML is further complicated by constraints on data privacy and fairness, as imposed by ethical and regulatory concerns like GDPR.\nThis workshop aims to bring together researchers and practitioners to discuss challenges for AI/ML in financial services, and the opportunities such challenges represent to research communities. The workshop will consist of invited talks, panel discussions and short paper presentations, which will showcase ongoing research and novel algorithms resulting from collaboration of AI/ML and cybersecurity communities, as well as the challenges that arise from applying these ideas in domain-specific contexts.\n\n", "speaker_info": [{"name": "Alina Oprea", "affilation": "Northeastern University"}, {"name": "Avigdor Gal", "affilation": "Technion -- Israel Institute of Technology"}, {"name": "Isabelle Moulinier", "affilation": "Capital One"}, {"name": "Jiahao Chen", "affilation": "Capital One"}, {"name": "Manuela Veloso", "affilation": "JPMorgan and Carnegie Mellon University"}, {"name": "Senthil Kumar", "affilation": "Capital One"}, {"name": "Tanveer Faruquie", "affilation": "Capital One"}]}, {"workshop_id": "13193", "workshop_name": "Safety and Robustness in Decision-making", "workshop_link": "https://sites.google.com/view/neurips19-safe-robust-workshop", "workshop_abstract": "Interacting with increasingly sophisticated decision-making systems is becoming more and more a part of our daily life. This creates an immense responsibility for designers of these systems to build them in a way to guarantee safe interaction with their users and good performance, in the presence of noise and changes in the environment, and/or of model misspecification and uncertainty. Any progress in this area will be a huge step forward in using decision-making algorithms in emerging high stakes applications, such as autonomous driving, robotics, power systems, health care, recommendation systems, and finance. \nThis workshop aims to bring together researchers from academia and industry in order to discuss main challenges, describe recent advances, and highlight future research directions pertaining to develop safe and robust decision-making systems. We aim to highlight new and emerging theoretical and applied research opportunities for the community that arise from the evolving needs for decision-making systems and algorithms that guarantee safe interaction and good performance under a wide range of uncertainties in the environment.\n\n", "speaker_info": [{"name": "Mohammad Ghavamzadeh", "affilation": "Facebook AI Research"}, {"name": "Shie Mannor", "affilation": "Technion"}, {"name": "Yisong Yue", "affilation": "Caltech"}, {"name": "Marek Petrik", "affilation": "University of New Hampshire"}, {"name": "Yinlam Chow", "affilation": "DeepMind"}]}, {"workshop_id": "13194", "workshop_name": "Science meets Engineering of Deep Learning", "workshop_link": "https://sites.google.com/corp/view/sedl-neurips-2019/introduction", "workshop_abstract": "Deep learning can still be a complex mix of art and engineering despite its tremendous success in recent years, and there is still progress to be made before it has fully evolved into a mature scientific discipline. The interdependence of architecture, data, and optimization gives rise to an enormous landscape of design and performance intricacies that are not well-understood. The evolution from engineering towards science in deep learning can be achieved by pushing the disciplinary boundaries. Unlike in the natural and physical sciences -- where experimental capabilities can hamper progress, i.e. limitations in what quantities can be probed and measured in physical systems, how much and how often -- \\textbf{in deep learning the vast majority of relevant quantities that we wish to measure can be tracked in some way.} As such, a greater limiting factor towards scientific understanding and principled design in deep learning is how to  \\textbf{insightfully harness the tremendous collective experimental capability of the field}. As a community, some primary aims would be to (i) \\emph{identify} obstacles to better models and algorithms, (ii) \\emph{identify} the general trends that are potentially important which we wish to understand scientifically and potentially theoretically and; (iii) \\emph{careful design} of scientific experiments whose purpose is to clearly \\emph{resolve and pinpoint} the origin of mysteries (so-called ``smoking-gun\" experiments).\n\n", "speaker_info": [{"name": "Levent Sagun", "affilation": "EPFL"}, {"name": "Caglar Gulcehre", "affilation": "Deepmind"}, {"name": "Adriana Romero", "affilation": "FAIR"}, {"name": "Negar Rostamzadeh", "affilation": "Element AI"}, {"name": "Nando de Freitas", "affilation": "DeepMind"}]}, {"workshop_id": "13195", "workshop_name": "Sets and Partitions", "workshop_link": "https://www.sets.parts", "workshop_abstract": "In most machine learning, the input size is fixed, as in classic classification problems such as digit recognition or speaker identification. In cases where the input size is not fixed,  inductive biases of the problem, such as sequence order or pixel position, often favor sequence models or convolutional neural networks. However, there are many tasks in natural language, computer vision, and other applications, where unordered set-based models are desired. Apart from these, recently set based data has found applications in experimental sciences like high energy physics, cosmology, crystallography, as well as by artists for generating creative 3d models. The use cases for set-based models can be endless, ranging from classification, regression, to even discovering meaningful partitions/clusterings of data in unsupervised/semi-supervised settings.  Moreover, with growing increase in data sizes, it is very useful to be able the organize datasets. One would want to a learn permutation and size invariant partition, possibly hierarchical partition, of the data which is human interpretable. Great progress has been made in traditional clustering and hierarchy learning with discrete operations, but joint learning of representations and hierarchy with gradient method seems to be have lot of opportunities touching interpretability, unsupervised learning, and scalability. As a given dataset in nothing but a set and overlapping techniques, we have a workshop on Sets and Partitions.\nOrganizing Committee:\nAndrew McCallum, UMass Amherst\nRuslan Salakhutdinov, CMU \nBarnabas Poczos, CMU \nJunier Oliva, UNC Chapel Hill \nManzil Zaheer, Google Research\nAri Kobren, UMass Amherst\nNicholas Monath, UMass Amherst\nwith senior advisory support from Alex Smola.\n\n", "speaker_info": [{"name": "Nicholas Monath", "affilation": "University of Massachusetts Amherst"}, {"name": "Manzil Zaheer", "affilation": "Google"}, {"name": "Andrew McCallum", "affilation": "UMass Amherst"}, {"name": "Ari Kobren", "affilation": "UMass Amherst"}, {"name": "Junier Oliva", "affilation": "UNC-Chapel Hill"}, {"name": "Barnabas Poczos", "affilation": "Carnegie Mellon University"}, {"name": "Ruslan Salakhutdinov", "affilation": "Carnegie Mellon University"}]}, {"workshop_id": "13199", "workshop_name": "The Optimization Foundations of Reinforcement Learning", "workshop_link": "https://optrl2019.github.io/", "workshop_abstract": "Interest in reinforcement learning (RL) has boomed with recent improvements in benchmark tasks that suggest the potential for a revolutionary advance in practical applications.  Unfortunately, research in RL remains hampered by limited theoretical understanding, making the field overly reliant on empirical exploration with insufficient principles to guide future development.  It is imperative to develop a stronger fundamental understanding of the success of recent RL methods, both to expand the useability of the methods and accelerate future deployment. Recently, fundamental concepts from optimization and control theory have provided a fresh perspective that has led to the development of sound RL algorithms with provable efficiency.  The goal of this workshop is to catalyze the growing synergy between RL and optimization research, promoting a rational reconsideration of the foundational principles for reinforcement learning, and bridging the gap between theory and practice.\n\n", "speaker_info": [{"name": "Bo Dai", "affilation": "Google Brain"}, {"name": "Niao He", "affilation": "UIUC"}, {"name": "Nicolas Le Roux", "affilation": "Google"}, {"name": "Lihong Li", "affilation": "Google Brain"}, {"name": "Dale Schuurmans", "affilation": "Google Inc."}, {"name": "Martha White", "affilation": "University of Alberta"}]}, {"workshop_id": "13203", "workshop_name": "Workshop on Human-Centric Machine Learning", "workshop_link": "https://sites.google.com/view/hcml-2019", "workshop_abstract": "The growing field of Human-centric ML seeks to minimize the potential harms, risks, and burdens of big data technologies on the public, and at the same time, maximize their societal benefits. In this workshop, we address a wide range of challenges from diverse, multi-disciplinary viewpoints. We bring together experts from a diverse set of backgrounds. Our speakers are leading experts in ML, human-computer interaction, ethics, and law. Each of our speakers will focus on one core human-centred challenge (namely, fairness, accountability, interpretability, transparency, security, and privacy) in specific application domains (such as medicine, welfare programs, governance, and regulation). One of the main goals of this workshop is to help the community understand where it stands after a few years of rapid technical development and identify promising research directions to pursue in the years to come. Our speakers identify in their presentations 3-5 research directions that they consider to be of crucial importance. These directions are further debated in one of our panel discussions.\n\n", "speaker_info": [{"name": "Plamen Angelov", "affilation": "Lancaster University"}, {"name": "Nuria Oliver", "affilation": "Data-Pop Alliance"}, {"name": "Adrian Weller", "affilation": "University of Cambridge"}, {"name": "Manuel Rodriguez", "affilation": "MPI SWS"}, {"name": "Isabel Valera", "affilation": "Max Planck Institute for Intelligent Systems"}, {"name": "Silvia Chiappa", "affilation": "DeepMind"}, {"name": "Hoda Heidari", "affilation": "ETH Z\u00fcrich"}, {"name": "Niki Kilbertus", "affilation": "Max Planck Institute for Intelligent Systems"}]}, {"workshop_id": "13200", "workshop_name": "The third Conversational AI workshop \u2013 today's practice and tomorrow's potential", "workshop_link": "http://alborz-geramifard.com/workshops/neurips19-Conversational-AI/Main.html", "workshop_abstract": "In the span of only a few years, conversational systems have become commonplace. Every day, millions of people use natural-language interfaces such as Siri, Google Now, Cortana, Alexa and others via in-home devices, phones, or messaging channels such as Messenger, Slack, Skype, among others. \u00a0At the same time, interest among the research community in conversational systems has blossomed: for supervised and reinforcement learning, conversational systems often serve as both a benchmark task and an inspiration for new ML methods at conferences which don't focus on speech and language per se, such as NIPS, ICML, IJCAI, and others. Such movement has not been unnoticed by major publications. This year in collaboration with AAAI community, the AI magazine will have a special issue on conversational AI (https://tinyurl.com/y6shq2ld). Moreover, research community challenge tasks are proliferating, including the seventh Dialog Systems Technology Challenge (DSTC7), the Amazon Alexa prize, and the Conversational Intelligence Challenge live competitions at NIPS (2017, 2018).\nFollowing the overwhelming participation in our last two NeurIPS workshops: \n2017: 9 invited talks, 26 submissions, 3 oral papers, 13 accepted papers, 37 reviewers\n2018: 4 invited talks, 42 submission, 6 oral papers, 23 accepted papers, 58 reviewers, we are excited to continue promoting cross-pollination of ideas between academic research centers and industry. The goal of this workshop is to bring together researchers and practitioners in this area, to clarify impactful research problems, understand well-founded methods, share findings from large-scale real-world deployments, and generate new ideas for future lines of research. \nThis one day workshop will include invited talks and a panel from academia and industry, contributed work, and open discussion.\n\n", "speaker_info": [{"name": "Alborz Geramifard", "affilation": "Facebook AI"}, {"name": "Jason Williams", "affilation": "Apple"}, {"name": "Matt Henderson", "affilation": "PolyAI"}, {"name": "Luis Lastras", "affilation": "IBM Research AI"}, {"name": "Dilek Hakkani-Tur", "affilation": "Amazon Alexa AI"}, {"name": "Mari Ostendorf", "affilation": "University of Washington"}, {"name": "Milica Gasic", "affilation": "University of Cambridge"}, {"name": "Asli Celikyilmaz", "affilation": "Microsoft Research"}]}, {"workshop_id": "13201", "workshop_name": "Visually Grounded Interaction and Language ", "workshop_link": "https://vigilworkshop.github.io", "workshop_abstract": "The dominant paradigm in modern natural language understanding is learning statistical language models from text-only corpora. This approach is founded on a distributional notion of semantics, i.e. that the \nmeaning'' of a word is based only on its relationship to other words. While effective for many applications, this approach suffers from limited semantic understanding -- symbols learned this way lack any concrete groundings into the multimodal, interactive environment in which communication takes place. The symbol grounding problem first highlighted this limitation, that\nmeaningless symbols (i.e. words) cannot be grounded in anything but other meaningless symbols''.\nOn the other hand, humans acquire language by communicating about and interacting within a rich, perceptual environment -- providing concrete groundings, e.g. to objects or concepts either physical or psychological. Thus, recent works have aimed to bridge computer vision, interactive learning, and natural language understanding through language learning tasks based on natural images or through embodied agents performing interactive tasks in physically simulated environments, often drawing on the recent successes of deep learning and reinforcement learning. We believe these lines of research pose a promising approach for building models that do grasp the world's underlying complexity.\nThe goal of this third ViGIL workshop is to bring together scientists from various backgrounds - machine learning, computer vision, natural language processing, neuroscience, cognitive science, psychology, and philosophy - to share their perspectives on grounding, embodiment, and interaction. By providing this opportunity for cross-discipline discussion, we hope to foster new ideas about how to learn and leverage grounding in machines as well as build new bridges between the science of human cognition and machine learning.\n\n", "speaker_info": [{"name": "Florian Strub", "affilation": "Univ Lille1, CRIStAL, Inria - SequeL Team"}, {"name": "Abhishek Das", "affilation": "Georgia Tech"}, {"name": "Erik Wijmans", "affilation": "Georgia Institute of Technology"}, {"name": "Harm de Vries", "affilation": "Universit\u00e9 de Montr\u00e9al"}, {"name": "Stefan Lee", "affilation": "Georgia Institute of Technology"}, {"name": "Alane Suhr", "affilation": "Cornell"}, {"name": "Drew Arad Hudson", "affilation": "Stanford University"}]}, {"workshop_id": "13204", "workshop_name": "ML For Systems", "workshop_link": "mlforsystems.org", "workshop_abstract": "Designing specialized hardware for deep learning is a topic that has received significant research attention, both in industrial and academic settings, leading to exponential increases in compute capability in GPUs and accelerators. However, using machine learning to optimize and accelerate software and hardware systems is a lightly explored but promising field, with broad implications for computing as a whole. Very recent work has outlined a broad scope where deep learning vastly outperforms traditional heuristics including topics such as: scheduling, data structure design, microarchitecture, compilers, and control of warehouse scale computing systems.\nThe focus of this workshop is to expand upon this recent work and build a community focused on using machine learning in computer systems problems. We seek to improve the state of the art in the areas where learning has already proven to perform better than traditional heuristics, as well as expand to new areas throughout the system stack such as hardware/circuit design and operating/runtime systems. We hope that by providing a formal venue for researchers from both fields to meet and interact, that the result will include both fundamental research in ML as well as real-world impact to computer systems design and implementation.\n\n", "speaker_info": [{"name": "Milad Hashemi", "affilation": "Google"}, {"name": "Azalia Mirhoseini", "affilation": "Google Brain"}, {"name": "Anna Goldie", "affilation": "Google Brain"}, {"name": "Kevin Swersky", "affilation": "Google"}, {"name": "Jonathan Raiman", "affilation": "OpenAI"}, {"name": "Xinlei XU", "affilation": "NYU"}]}, {"workshop_id": "13181", "workshop_name": "Meta-Learning", "workshop_link": "http://metalearning.ml/", "workshop_abstract": "Recent years have seen rapid progress in meta\u00adlearning methods, which learn (and optimize) the performance of learning methods based on data, generate new learning methods from scratch, and learn to transfer knowledge across tasks and domains. Meta\u00adlearning can be seen as the logical conclusion of the arc that machine learning has undergone in the last decade, from learning classifiers, to learning representations, and finally to learning algorithms that themselves acquire representations and classifiers. The ability to improve one\u2019s own learning capabilities through experience can also be viewed as a hallmark of intelligent beings, and there are strong connections with work on human learning in neuroscience. The goal of this workshop is to bring together researchers from all the different communities and topics that fall under the umbrella of meta\u00adlearning. We expect that the presence of these different communities will result in a fruitful exchange of ideas and stimulate an open discussion about the current challenges in meta\u00adlearning, as well as possible solutions.\n\n", "speaker_info": [{"name": "Roberto Calandra", "affilation": "Facebook AI Research"}, {"name": "Ignasi Clavera Gilaberte", "affilation": "UC Berkeley"}, {"name": "Frank Hutter", "affilation": "University of Freiburg"}, {"name": "Joaquin Vanschoren", "affilation": "Eindhoven University of Technology, OpenML"}, {"name": "Jane Wang", "affilation": "DeepMind"}]}, {"workshop_id": "13164", "workshop_name": "Machine Learning for Autonomous Driving", "workshop_link": "https://ml4ad.github.io/", "workshop_abstract": "Autonomous vehicles (AVs) provide a rich source of high-impact research problems for the machine learning (ML) community at NeurIPS in diverse fields including computer vision, probabilistic modeling, gesture recognition, pedestrian and vehicle forecasting, human-machine interaction, and multi-agent planning. The common goal of autonomous driving can catalyze discussion between these subfields, generating a cross-pollination of research ideas. Beyond the benefits to the research community, AV research can improve society by reducing road accidents; giving independence to those unable to drive; and inspiring younger generations towards ML with tangible examples of ML-based technology clearly visible on local streets.\nAs many NeurIPS attendees are key drivers behind AV-applied ML, the proposed NeurIPS 2019 Workshop on Autonomous Driving intends to bring researchers together from both academia and industries to discuss machine learning applications in autonomous driving. Our proposal includes regular paper presentations, invited speakers, and technical benchmark challenges to present the current state of the art, as well as the limitations and future directions for autonomous driving.\n\n", "speaker_info": [{"name": "Rowan McAllister", "affilation": "UC Berkeley"}, {"name": "Nick Rhinehart", "affilation": "Carnegie Mellon University"}, {"name": "Fisher Yu", "affilation": "Princeton University"}, {"name": "Li Erran Li", "affilation": "Pony.ai"}, {"name": "Anca Dragan", "affilation": "UC Berkeley"}]}, {"workshop_id": "13188", "workshop_name": "Program Transformations for ML", "workshop_link": "https://program-transformations.github.io/", "workshop_abstract": "Machine learning researchers often express complex models as a program, relying on program transformations to add functionality. New languages and transformations (e.g., TorchScript and TensorFlow AutoGraph) are becoming core capabilities of ML libraries. However, existing transformations, such as automatic differentiation (AD), inference in probabilistic programming languages (PPL), and optimizing compilers are often built in isolation, and limited in scope. This workshop aims at viewing program transformations in ML in a unified light, making these capabilities more accessible, and building entirely new ones. \nProgram transformations are an area of active study. AD transforms a program performing numerical computation into one computing the gradient of those computations. In PPL, a program describing a sampling procedure can be modified to perform inference on model parameters given observations. Other examples are vectorizing a program expressed on one data point, and learned transformations where ML models use programs as inputs or outputs.\nThis workshop will bring together researchers in the fields of AD, programming languages, compilers, and ML, with the goal of understanding the commonalities between disparate approaches and views, and sharing ways to make these techniques broadly available. It would enable ML practitioners to iterate faster on novel models and architectures (e.g., those naturally expressed through high-level constructs like recursion).\nTopics:\n\u2014Abstractions and syntax (beyond meta-programming and operator overloading) to naturally express a program (expression, or procedure) as an object to be manipulated.\n\u2014Techniques from AD and PPL the ML community could adopt to enable research on new models\n\u2014How to overcome challenges due to the ML\u2019s specific hardware (GPUs, specialized chips) and software (Python) stacks, and the particular demands of practitioners for their tools\n\u2014Greater collaboration between ML and programming languages communities\n\n", "speaker_info": [{"name": "Pascal Lamblin", "affilation": "Google"}, {"name": "Atilim Gunes Baydin", "affilation": "University of Oxford"}, {"name": "Alexander Wiltschko", "affilation": "Google Brain"}, {"name": "Bart van Merri\u00ebnboer", "affilation": "Google"}, {"name": "Emily Fertig", "affilation": "Google Brain"}, {"name": "Barak Pearlmutter", "affilation": "Maynooth University"}, {"name": "David Duvenaud", "affilation": "University of Toronto"}, {"name": "Laurent Hascoet", "affilation": "INRIA"}]}, {"workshop_id": "13191", "workshop_name": "Robot Learning: Control and Interaction in the Real World", "workshop_link": "www.robot-learning.ml", "workshop_abstract": "The growing capabilities of learning-based methods in control and robotics has precipitated a shift in the design of software for autonomous systems. Recent successes fuel the hope that robots will increasingly perform varying tasks working alongside humans in complex, dynamic environments. However, the application of learning approaches to real-world robotic systems has been limited because real-world scenarios introduce challenges that do not arise in simulation.\nIn this workshop, we aim to identify and tackle the main challenges to learning on real robotic systems. First, most machine learning methods rely on large quantities of labeled data. While raw sensor data is available at high rates, the required variety is hard to obtain and the human effort to annotate or design reward functions is an even larger burden. Second, algorithms must guarantee some measure of safety and robustness to be deployed in real systems that interact with property and people. Instantaneous reset mechanisms, as common in simulation to recover from even critical failures, present a great challenge to real robots. Third, the real world is significantly more complex and varied than curated datasets and simulations. Successful approaches must scale to this complexity and be able to adapt to novel situations. \n\n", "speaker_info": [{"name": "Markus Wulfmeier", "affilation": "DeepMind"}, {"name": "Roberto Calandra", "affilation": "Facebook AI Research"}, {"name": "Kate Rakelly", "affilation": "UC Berkeley"}, {"name": "Sanket Kamthe", "affilation": "Imperial College London"}, {"name": "Danica Kragic", "affilation": "KTH Royal Institute of Technology"}, {"name": "Stefan Schaal", "affilation": "Google"}]}, {"workshop_id": "13176", "workshop_name": "\u201cDo the right thing\u201d: machine learning and causal inference for improved decision making", "workshop_link": "http://tripods.cis.cornell.edu/neurips19_causalml/", "workshop_abstract": "In recent years, machine learning has seen important advances in its theoretical and practical domains, with some of the most significant applications in online marketing and commerce, personalized medicine, and data-driven policy-making. This dramatic success has led to increased expectations for autonomous systems to make the right decision at the right target at the right time.  This gives rise to one of the major challenges of machine learning today that is the understanding of the cause-effect connection. Indeed, actions, intervention, and decisions have important consequences, and so, in seeking to make the best decision, one must understand the process of identifying causality.   By embracing causal reasoning autonomous systems will be able to answer counterfactual questions, such as \u201cWhat if I had treated a patient differently?\u201d, and \u201cWhat if had ranked a list differently?\u201d thus helping to establish the evidence base for important decision-making processes. \nThe purpose of this workshop is to bring together experts from different fields to discuss the relationships between machine learning and causal inference and to discuss and highlight the formalization and algorithmization of causality toward achieving human-level machine intelligence.\nThis purpose will guide the makeup of the invited talks and the topics for the panel discussions. The panel discussions will tackle controversial topics, with the intent of drawing out an engaging intellectual debate and conversation across fields. \nThis workshop will lead to advance and extend knowledge on how machine learning could be used to conduct causal inference, and how causal inference could support the development of machine learning methods for improved decision-making.  \n\n", "speaker_info": [{"name": "Michele Santacatterina", "affilation": "TRIPODS Center for Data Science - Cornell University"}, {"name": "Thorsten Joachims", "affilation": "Cornell"}, {"name": "Nathan Kallus", "affilation": "Cornell University"}, {"name": "Adith Swaminathan", "affilation": "Microsoft Research"}, {"name": "David Sontag", "affilation": "MIT"}, {"name": "Angela Zhou", "affilation": "Cornell University"}]}, {"workshop_id": "13202", "workshop_name": "Workshop on Federated Learning for Data Privacy and Confidentiality", "workshop_link": "http://federated-learning.org/fl-neurips-2019/", "workshop_abstract": "Privacy and security are becoming major concerns in recent years, particularly as companies and organizations are collecting increasingly detailed information about their products and users. This information can enable machine learning that produces more helpful products. However, at the same time, it expands the potential for misuse, and increases corresponding public concerns about the way companies use data, particularly when private data about individuals is involved. Recent research shows that privacy and utility do not necessarily need to be at odds, but can be addressed by careful design and analysis.\nA specific approach that has the potential to address a number of problems in this space and has been gaining attention in the industry, is Federated Learning. At this moment, the pace of research innovation in federated learning is hampered by the relative complexity of properly setting up even simple experiments that reflect the practical setting. This issue is exacerbated in academic settings which typically lack access to actual user data. This workshop aims to make it easier to be productive in this area or research, by promoting recently open-sourced projects designed with this problem in mind. It will provide an opportunity to share the most recent and innovative work in this area, and discuss open problems and relevant approaches.\n\n", "speaker_info": [{"name": "Lixin Fan", "affilation": "Nokia Technologies"}, {"name": "Jakub Kone\u010dn\u00fd", "affilation": "Google Research"}, {"name": "Yang Liu", "affilation": "Webank"}, {"name": "Brendan McMahan", "affilation": "Google"}, {"name": "Virginia Smith", "affilation": "Carnegie Mellon University"}, {"name": "Han Yu", "affilation": "Nanyang Technological University (NTU)"}]}, {"workshop_id": "13196", "workshop_name": "Shared Visual Representations in Human and Machine Intelligence", "workshop_link": "http://www.svrhm2019.com/", "workshop_abstract": "The goal of the Shared Visual Representations in Human and Machine Intelligence workshop is to disseminate relevant, parallel findings in the fields of computational neuroscience, psychology, and cognitive science that may inform modern machine learning methods. In the past few years, machine learning methods---especially deep neural networks---have widely permeated the vision science, cognitive science, and neuroscience communities.\nAs a result, scientific modeling in these fields has greatly benefited, producing a swath of potentially critical new insights into human learning and intelligence, which remains the gold standard for many tasks. However,\nthe machine learning community has been largely unaware of these cross-disciplinary insights and analytical tools, which may help to solve many of the current problems that ML theorists and engineers face today (\\textit{e.g.,}  adversarial attacks, compression, continual learning,\nand unsupervised learning).\nThus we propose to invite leading cognitive scientists with strong computational backgrounds to disseminate their findings to the machine learning community with the hope of closing the loop by nourishing new ideas and creating cross-disciplinary collaborations.\n\n", "speaker_info": [{"name": "Arturo Deza", "affilation": "Harvard University"}, {"name": "Joshua Peterson", "affilation": "UC Berkeley"}, {"name": "Apurva Ratan Murty", "affilation": "Massachusetts Institute of Technology"}, {"name": "Thomas Griffiths", "affilation": "Princeton University"}]}, {"workshop_id": "13197", "workshop_name": "Solving inverse problems with deep networks: New architectures, theoretical foundations, and applications", "workshop_link": "https://deep-inverse.org", "workshop_abstract": "There is a long history of algorithmic development for solving inverse problems arising in sensing and imaging systems and beyond. Examples include medical and computational imaging, compressive sensing, as well as community detection in networks. Until recently, most algorithms for solving inverse problems in the imaging and network sciences were based on static signal models derived from physics or intuition, such as wavelets or sparse representations.\nToday, the best performing approaches for the aforementioned image reconstruction and sensing problems are based on deep learning, which learn various elements of the method including i) signal representations, ii) stepsizes and parameters of iterative algorithms, iii) regularizers, and iv) entire inverse functions. For example, it has recently been shown that solving a variety of inverse problems by transforming an iterative, physics-based algorithm into a deep network whose parameters can be learned from training data, offers faster convergence and/or a better quality solution. Moreover, even with very little or no learning, deep neural networks enable superior performance for classical linear inverse problems such as denoising and compressive sensing. Motivated by those success stories, researchers are redesigning traditional imaging and sensing systems.\nHowever, the field is mostly wide open with a range of theoretical and practical questions unanswered. In particular, deep-neural network based approaches often lack the guarantees of the traditional physics based methods, and while typically superior can make drastic reconstruction errors, such as fantasizing a tumor in an MRI reconstruction.\nThis workshop aims at bringing together theoreticians and practitioners in order to chart out recent advances and discuss new directions in deep neural network based approaches for solving inverse problems in the imaging and network sciences.\n\n", "speaker_info": [{"name": "Reinhard Heckel", "affilation": "TUM"}, {"name": "Paul Hand", "affilation": "Northeastern University"}, {"name": "Richard Baraniuk", "affilation": "Rice University"}, {"name": "Joan Bruna", "affilation": "NYU"}, {"name": "Alex Dimakis", "affilation": "University of Texas, Austin"}, {"name": "Deanna Needell", "affilation": "UCLA"}]}]